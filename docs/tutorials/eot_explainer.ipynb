{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c02efd2d",
   "metadata": {},
   "source": [
    "# EOTExplainer: Entropic Optimal Transport\n",
    "\n",
    "This tutorial covers the `EOTExplainer`, which uses entropic optimal transport for more flexible feature importance computation.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. When to use EOTExplainer over OTExplainer\n",
    "2. Adaptive epsilon selection\n",
    "3. Stochastic transport sampling\n",
    "4. Handling mixed-type data with Gower distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22022569",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dfi.explainers import EOTExplainer, OTExplainer\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1fdd30",
   "metadata": {},
   "source": [
    "## Why Entropic OT?\n",
    "\n",
    "Gaussian OT assumes your data follows a Gaussian distribution. When this assumption is violated (e.g., multimodal data, heavy tails), **Entropic OT** provides a more flexible alternative.\n",
    "\n",
    "EOTExplainer uses the **Sinkhorn algorithm** to solve:\n",
    "\n",
    "$$P^* = \\arg\\min_P \\langle C, P \\rangle + \\epsilon H(P)$$\n",
    "\n",
    "where $C$ is the cost matrix, $P$ is the transport plan, and $H$ is entropy regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa03543",
   "metadata": {},
   "source": [
    "## Example: Non-Gaussian Data\n",
    "\n",
    "Let's create bimodal data where Gaussian OT may struggle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ec974f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bimodal data\n",
    "n_samples = 300\n",
    "n_features = 5\n",
    "\n",
    "# Mix of two Gaussian clusters\n",
    "cluster1 = np.random.randn(n_samples // 2, n_features) - 2\n",
    "cluster2 = np.random.randn(n_samples // 2, n_features) + 2\n",
    "X_train = np.vstack([cluster1, cluster2])\n",
    "\n",
    "# Shuffle\n",
    "np.random.shuffle(X_train)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(X_train[:, 0], bins=30, edgecolor='black')\n",
    "plt.title(\"Feature 0 Distribution (Bimodal)\")\n",
    "plt.xlabel(\"Value\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], alpha=0.5, s=10)\n",
    "plt.xlabel(\"X₀\")\n",
    "plt.ylabel(\"X₁\")\n",
    "plt.title(\"First Two Features\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bd4e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model: feature 0 is most important\n",
    "def model(X):\n",
    "    return X[:, 0] ** 2 + 0.5 * X[:, 1]\n",
    "\n",
    "X_test = np.vstack([\n",
    "    np.random.randn(25, n_features) - 2,\n",
    "    np.random.randn(25, n_features) + 2,\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ea52c7",
   "metadata": {},
   "source": [
    "## Compare OT vs EOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c869c63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian OT\n",
    "ot_explainer = OTExplainer(model, data=X_train, nsamples=50)\n",
    "ot_results = ot_explainer(X_test)\n",
    "\n",
    "# Entropic OT with auto epsilon\n",
    "eot_explainer = EOTExplainer(\n",
    "    model, \n",
    "    data=X_train, \n",
    "    nsamples=50,\n",
    "    auto_epsilon=True,\n",
    ")\n",
    "eot_results = eot_explainer(X_test)\n",
    "\n",
    "# Compare\n",
    "print(\"Feature importance comparison:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"{'Feature':>8} {'OT':>12} {'EOT':>12}\")\n",
    "print(\"-\" * 40)\n",
    "for i in range(n_features):\n",
    "    print(f\"{i:>8} {ot_results['phi_X'][i]:>12.4f} {eot_results['phi_X'][i]:>12.4f}\")\n",
    "\n",
    "print(f\"\\nAuto-selected epsilon: {eot_explainer.epsilon:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003a7223",
   "metadata": {},
   "source": [
    "## Epsilon Selection\n",
    "\n",
    "The `epsilon` parameter controls the entropy regularization:\n",
    "- **Small epsilon**: Sharp transport (closer to true OT), may be unstable\n",
    "- **Large epsilon**: Smooth transport, loses structure\n",
    "\n",
    "Use `auto_epsilon=True` to automatically select based on median pairwise distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050ab946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different epsilon values\n",
    "epsilon_values = [0.01, 0.1, 1.0, 10.0]\n",
    "results_by_eps = {}\n",
    "\n",
    "for eps in epsilon_values:\n",
    "    explainer = EOTExplainer(\n",
    "        model, data=X_train, nsamples=50,\n",
    "        epsilon=eps, auto_epsilon=False\n",
    "    )\n",
    "    results_by_eps[eps] = explainer(X_test)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, len(epsilon_values), figsize=(14, 3))\n",
    "for ax, eps in zip(axes, epsilon_values):\n",
    "    phi = results_by_eps[eps][\"phi_X\"]\n",
    "    ax.bar(range(n_features), phi)\n",
    "    ax.set_xlabel(\"Feature\")\n",
    "    ax.set_ylabel(\"Importance\")\n",
    "    ax.set_title(f\"ε = {eps}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45e88c2",
   "metadata": {},
   "source": [
    "## Stochastic Transport Sampling\n",
    "\n",
    "Instead of using the barycentric map (average transport), you can sample from the transport kernel for variance reduction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2241247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without stochastic transport\n",
    "explainer_det = EOTExplainer(\n",
    "    model, data=X_train, nsamples=50,\n",
    "    auto_epsilon=True,\n",
    "    stochastic_transport=False,\n",
    ")\n",
    "results_det = explainer_det(X_test)\n",
    "\n",
    "# With stochastic transport\n",
    "explainer_stoch = EOTExplainer(\n",
    "    model, data=X_train, nsamples=50,\n",
    "    auto_epsilon=True,\n",
    "    stochastic_transport=True,\n",
    "    n_transport_samples=10,\n",
    ")\n",
    "results_stoch = explainer_stoch(X_test)\n",
    "\n",
    "print(\"Feature importance with stochastic transport:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Feature':>8} {'Deterministic':>14} {'Stochastic':>14}\")\n",
    "print(\"-\" * 50)\n",
    "for i in range(n_features):\n",
    "    print(f\"{i:>8} {results_det['phi_X'][i]:>14.4f} {results_stoch['phi_X'][i]:>14.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d20c3c",
   "metadata": {},
   "source": [
    "## Mixed Data Types with Gower Distance\n",
    "\n",
    "EOTExplainer supports categorical features using Gower distance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26149aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mixed-type data\n",
    "n_samples = 200\n",
    "\n",
    "# Continuous features\n",
    "X_cont = np.random.randn(n_samples, 3)\n",
    "\n",
    "# Binary feature\n",
    "X_binary = np.random.choice([0, 1], size=(n_samples, 1))\n",
    "\n",
    "# Categorical feature (encoded as integers)\n",
    "X_cat = np.random.choice([0, 1, 2], size=(n_samples, 1))\n",
    "\n",
    "# Combine\n",
    "X_mixed = np.hstack([X_cont, X_binary, X_cat])\n",
    "print(f\"Mixed data shape: {X_mixed.shape}\")\n",
    "print(f\"Feature types: [continuous, continuous, continuous, binary, categorical]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13007a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model that uses all features\n",
    "def mixed_model(X):\n",
    "    # Continuous contribution\n",
    "    y = X[:, 0] + 0.5 * X[:, 1]\n",
    "    # Binary feature effect\n",
    "    y += 2 * X[:, 3]\n",
    "    # Categorical feature effect\n",
    "    y += X[:, 4] * 0.3\n",
    "    return y\n",
    "\n",
    "X_test_mixed = np.hstack([\n",
    "    np.random.randn(30, 3),\n",
    "    np.random.choice([0, 1], size=(30, 1)),\n",
    "    np.random.choice([0, 1, 2], size=(30, 1)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af05493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Gower distance for mixed types\n",
    "import numpy as np\n",
    "feature_types = np.array([\"continuous\", \"continuous\", \"continuous\", \"binary\", \"categorical\"])\n",
    "\n",
    "explainer_gower = EOTExplainer(\n",
    "    mixed_model,\n",
    "    data=X_mixed,\n",
    "    nsamples=50,\n",
    "    cost_metric=\"gower\",\n",
    "    feature_types=feature_types,\n",
    "    auto_epsilon=True,\n",
    ")\n",
    "\n",
    "results_mixed = explainer_gower(X_test_mixed)\n",
    "\n",
    "print(\"Feature importance for mixed-type data:\")\n",
    "feature_names = [\"cont_0\", \"cont_1\", \"cont_2\", \"binary\", \"categorical\"]\n",
    "for i, name in enumerate(feature_names):\n",
    "    print(f\"  {name}: {results_mixed['phi_X'][i]:.4f}\")\n",
    "\n",
    "print(f\"\\nDetected types: {explainer_gower.detected_types_['types']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee842231",
   "metadata": {},
   "source": [
    "## Target Distribution: Gaussian vs Empirical\n",
    "\n",
    "EOTExplainer can use either a Gaussian target or an empirical (permuted) target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8207d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian target\n",
    "explainer_gaussian = EOTExplainer(\n",
    "    model, data=X_train, nsamples=50,\n",
    "    target=\"gaussian\",\n",
    "    auto_epsilon=True,\n",
    ")\n",
    "\n",
    "# Empirical target\n",
    "explainer_empirical = EOTExplainer(\n",
    "    model, data=X_train, nsamples=50,\n",
    "    target=\"empirical\",\n",
    "    auto_epsilon=True,\n",
    ")\n",
    "\n",
    "results_g = explainer_gaussian(X_test)\n",
    "results_e = explainer_empirical(X_test)\n",
    "\n",
    "print(\"Target distribution comparison:\")\n",
    "print(\"-\" * 45)\n",
    "print(f\"{'Feature':>8} {'Gaussian':>14} {'Empirical':>14}\")\n",
    "print(\"-\" * 45)\n",
    "for i in range(n_features):\n",
    "    print(f\"{i:>8} {results_g['phi_X'][i]:>14.4f} {results_e['phi_X'][i]:>14.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872aba4b",
   "metadata": {},
   "source": [
    "## Best Practices for EOTExplainer\n",
    "\n",
    "```python\n",
    "# Recommended settings for most cases\n",
    "explainer = EOTExplainer(\n",
    "    model,\n",
    "    data=X_train,\n",
    "    nsamples=50,\n",
    "    auto_epsilon=True,        # Let the algorithm choose\n",
    "    target=\"gaussian\",        # Standard choice\n",
    "    stochastic_transport=False,  # Enable for variance reduction\n",
    ")\n",
    "\n",
    "# For mixed-type data\n",
    "explainer = EOTExplainer(\n",
    "    model,\n",
    "    data=X_train,\n",
    "    cost_metric=\"gower\",      # or \"auto\"\n",
    "    categorical_threshold=10,  # Unique values ≤ 10 = categorical\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d89628e",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Key takeaways:\n",
    "\n",
    "1. **EOTExplainer** is more flexible than OTExplainer for non-Gaussian data\n",
    "2. Use `auto_epsilon=True` for automatic regularization tuning\n",
    "3. Enable `stochastic_transport=True` for variance reduction\n",
    "4. Use `cost_metric=\"gower\"` for mixed continuous/categorical features\n",
    "5. Choose `target=\"empirical\"` if you want to stay close to the data distribution"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
