{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64771235",
   "metadata": {},
   "source": [
    "# OTExplainer: Gaussian Optimal Transport\n",
    "\n",
    "This tutorial provides a deep dive into the `OTExplainer`, which uses Gaussian optimal transport for computing feature importance.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. Mathematical foundation of Gaussian OT\n",
    "2. Key hyperparameters and their effects\n",
    "3. When to use OTExplainer vs other methods\n",
    "4. Best practices for real-world usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6990e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dfi.explainers import OTExplainer\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c051506a",
   "metadata": {},
   "source": [
    "## Mathematical Background\n",
    "\n",
    "The key insight of OTExplainer is to use the **Gaussian optimal transport map** to create counterfactual distributions.\n",
    "\n",
    "Given data $X$ with mean $\\mu$ and covariance $\\Sigma$, we compute:\n",
    "\n",
    "$$Z = L^{-1}(X - \\mu)$$\n",
    "\n",
    "where $\\Sigma = LL^T$ (Cholesky decomposition). In Z-space, features are uncorrelated.\n",
    "\n",
    "To measure the importance of feature $j$, we:\n",
    "1. Replace $Z_j$ with an independent sample from $N(0, 1)$\n",
    "2. Transform back to X-space\n",
    "3. Compare model outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a59f3d",
   "metadata": {},
   "source": [
    "## Setup: Create Correlated Data\n",
    "\n",
    "Let's create data with correlated features to see how OTExplainer handles dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04031dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create covariance matrix with correlations\n",
    "n_features = 5\n",
    "correlation = 0.7\n",
    "\n",
    "cov_matrix = np.eye(n_features)\n",
    "cov_matrix[0, 1] = cov_matrix[1, 0] = correlation  # Features 0 and 1 are correlated\n",
    "cov_matrix[2, 3] = cov_matrix[3, 2] = correlation  # Features 2 and 3 are correlated\n",
    "\n",
    "# Generate correlated data\n",
    "n_samples = 500\n",
    "X_train = np.random.multivariate_normal(\n",
    "    mean=np.zeros(n_features),\n",
    "    cov=cov_matrix,\n",
    "    size=n_samples\n",
    ")\n",
    "\n",
    "print(\"Correlation matrix of training data:\")\n",
    "print(np.corrcoef(X_train.T).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88e79e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model: only feature 0 matters\n",
    "def model(X):\n",
    "    return X[:, 0] ** 2\n",
    "\n",
    "# Test data\n",
    "X_test = np.random.multivariate_normal(\n",
    "    mean=np.zeros(n_features),\n",
    "    cov=cov_matrix,\n",
    "    size=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c805f8df",
   "metadata": {},
   "source": [
    "## Effect of nsamples\n",
    "\n",
    "The `nsamples` parameter controls Monte Carlo variance. Let's see its effect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0dc14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different nsamples values\n",
    "nsamples_values = [10, 50, 200]\n",
    "results_by_nsamples = {}\n",
    "\n",
    "for ns in nsamples_values:\n",
    "    explainer = OTExplainer(model, data=X_train, nsamples=ns)\n",
    "    results_by_nsamples[ns] = explainer(X_test)\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(1, len(nsamples_values), figsize=(12, 4))\n",
    "for ax, ns in zip(axes, nsamples_values):\n",
    "    phi = results_by_nsamples[ns][\"phi_X\"]\n",
    "    se = results_by_nsamples[ns][\"se_X\"]\n",
    "    \n",
    "    ax.bar(range(n_features), phi, yerr=1.96*se, capsize=3)\n",
    "    ax.set_xlabel(\"Feature\")\n",
    "    ax.set_ylabel(\"Importance\")\n",
    "    ax.set_title(f\"nsamples={ns}\")\n",
    "    ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccc9e70",
   "metadata": {},
   "source": [
    "**Key observation**: Higher `nsamples` gives smaller error bars (lower variance) but takes longer to compute."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4df2f5",
   "metadata": {},
   "source": [
    "## Effect of sampling_method\n",
    "\n",
    "OTExplainer supports three sampling methods for counterfactual generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51487042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare sampling methods\n",
    "sampling_methods = [\"resample\", \"permutation\", \"normal\"]\n",
    "results_by_method = {}\n",
    "\n",
    "for method in sampling_methods:\n",
    "    explainer = OTExplainer(\n",
    "        model, \n",
    "        data=X_train, \n",
    "        nsamples=100,\n",
    "        sampling_method=method\n",
    "    )\n",
    "    results_by_method[method] = explainer(X_test)\n",
    "\n",
    "# Compare results\n",
    "print(\"Feature importance by sampling method:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Feature':>8}\", end=\"\")\n",
    "for method in sampling_methods:\n",
    "    print(f\"{method:>12}\", end=\"\")\n",
    "print()\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for i in range(n_features):\n",
    "    print(f\"{i:>8}\", end=\"\")\n",
    "    for method in sampling_methods:\n",
    "        phi = results_by_method[method][\"phi_X\"][i]\n",
    "        print(f\"{phi:>12.4f}\", end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3e40a2",
   "metadata": {},
   "source": [
    "**Sampling methods explained:**\n",
    "\n",
    "- `resample`: Sample from the background data (preserves marginal distribution)\n",
    "- `permutation`: Permute values within test set (no new values introduced)\n",
    "- `normal`: Sample from standard normal (strongest Gaussian assumption)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a506c2ce",
   "metadata": {},
   "source": [
    "## Visualizing the Z-space Transformation\n",
    "\n",
    "Let's visualize how OTExplainer transforms correlated data to uncorrelated Z-space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9642eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create explainer and access internal matrices\n",
    "explainer = OTExplainer(model, data=X_train, nsamples=50)\n",
    "\n",
    "# Transform to Z-space\n",
    "Z_train = (X_train - explainer.mean) @ explainer.L_inv\n",
    "\n",
    "# Plot X-space vs Z-space\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "# X-space (correlated)\n",
    "axes[0].scatter(X_train[:, 0], X_train[:, 1], alpha=0.5, s=10)\n",
    "axes[0].set_xlabel(\"X₀\")\n",
    "axes[0].set_ylabel(\"X₁\")\n",
    "axes[0].set_title(f\"X-space (correlation = {np.corrcoef(X_train[:, 0], X_train[:, 1])[0, 1]:.2f})\")\n",
    "axes[0].axis('equal')\n",
    "\n",
    "# Z-space (uncorrelated)\n",
    "axes[1].scatter(Z_train[:, 0], Z_train[:, 1], alpha=0.5, s=10)\n",
    "axes[1].set_xlabel(\"Z₀\")\n",
    "axes[1].set_ylabel(\"Z₁\")\n",
    "axes[1].set_title(f\"Z-space (correlation = {np.corrcoef(Z_train[:, 0], Z_train[:, 1])[0, 1]:.2f})\")\n",
    "axes[1].axis('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97247a77",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "### When to use OTExplainer\n",
    "\n",
    "✅ **Good for:**\n",
    "- Continuous features\n",
    "- Roughly Gaussian data\n",
    "- Fast computation\n",
    "- Stable results\n",
    "\n",
    "❌ **Consider EOTExplainer for:**\n",
    "- Heavily non-Gaussian data\n",
    "- Multimodal distributions\n",
    "- Mixed categorical/continuous features\n",
    "\n",
    "### Recommended settings\n",
    "\n",
    "```python\n",
    "explainer = OTExplainer(\n",
    "    model,\n",
    "    data=X_train,\n",
    "    nsamples=50,               # Good balance of speed/accuracy\n",
    "    sampling_method=\"resample\", # Preserves marginal distribution\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd67813",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Key takeaways:\n",
    "\n",
    "1. OTExplainer uses Gaussian OT to disentangle correlated features\n",
    "2. Higher `nsamples` reduces variance at the cost of computation time\n",
    "3. `sampling_method=\"resample\"` is recommended for most cases\n",
    "4. The transformation to Z-space removes correlations for clean attribution"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
